% This file was created with JabRef 2.10.
% Encoding: UTF-8

@article{mysore2021train,
  title={How to train your quadrotor: A framework for consistently smooth and responsive flight control via reinforcement learning},
  author={Mysore, Siddharth and Mabsout, Bassel and Saenko, Kate and Mancuso, Renato},
  journal={ACM Transactions on Cyber-Physical Systems (TCPS)},
  volume={5},
  number={4},
  pages={1--24},
  year={2021},
  publisher={ACM New York, NY}
}

@INPROCEEDINGS{mysore2021caps,
    author={Mysore, Siddharth and Mabsout, Bassel and Mancuso, Renato and Saenko, Kate},
    booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
    title={Regularizing Action Policies for Smooth Control with Reinforcement Learning}, 
    year={2021},
    volume={},
    number={},
    pages={1810-1816},
    doi={10.1109/ICRA48506.2021.9561138}
}

@article{adaptiveRLAerial,
    author = {Barzegar, Ali and Lee, Deokjin},
    year = {2022},
    month = {05},
    pages = {4764},
    title = {Deep Reinforcement Learning-Based Adaptive Controller for Trajectory Tracking and Altitude Control of an Aerial Robot},
    volume = {12},
    journal = {Applied Sciences},
    doi = {10.3390/app12094764}
}

@article{NFori,
  author    = {William Koch and
               Renato Mancuso and
               Richard West and
               Azer Bestavros},
  title     = {Reinforcement Learning for {UAV} Attitude Control},
  journal   = {ACM Transactions on Cyber-Physical Systems},
  year      = {2018}
}

@article{NFv2,
  author    = {William Koch and
               Renato Mancuso and
               Azer Bestavros},
  title     = {Neuroflight: Next Generation Flight Control Firmware},
  journal   = {CoRR},
  volume    = {abs/1901.06553},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.06553},
  archivePrefix = {arXiv},
  eprint    = {1901.06553},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-06553.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{betaflight-homepage, title={Betaflight}, url={https://betaflight.com/}}

@misc{supplementary, title={Swappable Neural Network Flight control System Homepage}, url={https://github.com/BU-Cyber-Physical-Systems-Lab/SwaNNFS}}

@misc{NFThesis,
    title={Flight Controller Synthesis Via Deep Reinforcement Learning},
    author={William Koch},
    year={2019},
    eprint={1909.06493},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@InProceedings{TRPO,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  year = 	 {2015},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}

@article{PPO,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {arXiv:1707.06347},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {arXiv:1707.06347},
}

@article{Qlearning,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@article{
    SAC,
    title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
    author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
    journal={International Conference on Machine Learning (ICML)},
    year={2018}
}

@article{DDPG,
  author    = {Timothy P. Lillicrap and
               Jonathan J. Hunt and
               Alexander Pritzel and
               Nicolas Heess and
               Tom Erez and
               Yuval Tassa and
               David Silver and
               Daan Wierstra},
  title     = {Continuous control with deep reinforcement learning},
  journal   = {International Conference on Learning Representations},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1509.02971},
}

@inproceedings{
    TD3,
    title={Addressing Function Approximation Error in Actor-Critic Methods},
    author={Fujimoto, Scott and Hoof, Herke and Meger, David},
    booktitle={International Conference on Machine Learning},
    pages={1587--1596},
    year={2018}
}

@misc{baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@misc{rl-zoo,
  author = {},
  title = {RL Hyperparameter Zoo},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/araffin/rl-baselines-zoo/tree/master/hyperparams}},
}



@misc{stable-baselines,
  author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {Stable Baselines},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
}

@inproceedings{henderson2018deep,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{islam2017reproducibility,
  title={Reproducibility of benchmarked deep reinforcement learning tasks for continuous control},
  author={Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina},
  journal={arXiv preprint arXiv:1708.04133},
  year={2017}
}

@inproceedings{
    Engstrom2020Implementation,
    title={Implementation Matters in Deep RL: A Case Study on PPO and TRPO},
    author={Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
    booktitle={International Conference on Learning Representations},
    year={2020},
}

@article{SpinningUp2018,
    author = {Achiam, Joshua},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018}
}


%--------------

@inproceedings{zhao2020sim,
  title={Sim-to-real transfer in deep reinforcement learning for robotics: a survey},
  author={Zhao, Wenshuai and Queralta, Jorge Pe{\~n}a and Westerlund, Tomi},
  booktitle={2020 IEEE Symposium Series on Computational Intelligence (SSCI)},
  pages={737--744},
  year={2020},
  organization={IEEE}
}

@inproceedings{golemo2018sim,
  title={Sim-to-real transfer with neural-augmented robot simulation},
  author={Golemo, Florian and Taiga, Adrien Ali and Courville, Aaron and Oudeyer, Pierre-Yves},
  booktitle={Conference on Robot Learning},
  pages={817--828},
  year={2018},
  organization={PMLR}
}

@inproceedings{yu2019sim,
  title={Sim-to-real transfer for biped locomotion},
  author={Yu, Wenhao and Kumar, Visak CV and Turk, Greg and Liu, C Karen},
  booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3503--3510},
  year={2019},
  organization={IEEE}
}

@inproceedings{bozhinoski2016leveraging,
  title={Leveraging collective run-time adaptation for uav-based systems},
  author={Bozhinoski, Darko and Bucchiarone, Antonio and Malavolta, Ivano and Marconi, Annapaola and Pelliccione, Patrizio},
  booktitle={2016 42th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)},
  pages={214--221},
  year={2016},
  organization={IEEE}
}

@misc{de2011flexible,
  title={Flexible data-centric UAV platform eases mission adaptation. White paper, Available online},
  author={de Jong, E},
  year={2011}
}

@article{wu2016drone,
  title={Drone streaming with Wi-Fi grid aggregation for virtual tour},
  author={Wu, Chenglei and Wang, Zhi and Yang, Shiqiang},
  journal={arXiv preprint arXiv:1605.09486},
  year={2016}
}

@IEEEtranBSTCTL{IEEEexample:BSTcontrol, CTLdash_repeated_names = "no"
}

@inproceedings{mysore2022multicritic,
    title={Multi-Critic Actor Learning: Teaching {RL} Policies to Act with Style},
    author={Mysore, Siddharth and Cheng, George and Zhao, Yunqi and Saenko, Kate and Wu, Meng},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=rJvY_5OzoI},
    note={accepted}
}

@article{benchmarkingRobo,
  author    = {A. Rupam Mahmood and
               Dmytro Korenkevych and
               Gautham Vasan and
               William Ma and
               James Bergstra},
  title     = {Benchmarking Reinforcement Learning Algorithms on Real-World Robots},
  journal   = {Conference on Robot Learning},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.07731},
  archivePrefix = {arXiv},
  eprint    = {1809.07731},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-07731.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Sim2multi,
  author    = {Artem Molchanov and
               Tao Chen and
               Wolfgang H{\"{o}}nig and
               James A. Preiss and
               Nora Ayanian and
               Gaurav S. Sukhatme},
  title     = {Sim-to-(Multi)-Real: Transfer of Low-Level Robust Control Policies
               to Multiple Quadrotors},
  journal   = {International Conference on Intelligent Robots and Systems},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.04628},
  archivePrefix = {arXiv},
  eprint    = {1903.04628},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-04628.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{SuttonBarto, 
  author = {Sutton, Richard S. and Barto, Andrew G.}, 
  title = {Introduction to Reinforcement Learning}, 
  year = {1998}, 
  isbn = {0262193981}, 
  publisher = {MIT Press}, 
  address = {Cambridge, MA, USA}, 
  edition = {1st} 
}

@article{Overfitting2,
  title={A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning},
  author={Amy X. Zhang and Nicolas Ballas and Joelle Pineau},
  journal={CoRR},
  year={2018},
  volume={abs/1806.07937}
}

@article{Overfitting,
  title={A Study on Overfitting in Deep Reinforcement Learning},
  author={Chiyuan Zhang and Oriol Vinyals and R{\'e}mi Munos and Samy Bengio},
  journal={CoRR},
  year={2018},
  volume={abs/1804.06893}
}

@article{Sim2Real,
  author    = {Fereshteh Sadeghi and
               Alexander Toshev and
               Eric Jang and
               Sergey Levine},
  title     = {{Sim2Real} View Invariant Visual Servoing by Recurrent Control},
  journal   = {CoRR},
  volume    = {abs/1712.07642},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1712.07642},
  timestamp = {Mon, 13 Aug 2018 16:46:34 +0200}, 
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@article{GYM,
  author    = {Greg Brockman and
               Vicki Cheung and
               Ludwig Pettersson and
               Jonas Schneider and
               John Schulman and
               Jie Tang and
               Wojciech Zaremba},
  title     = {OpenAI Gym},
  journal   = {CoRR},
  volume    = {abs/1606.01540},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1606.01540},
  timestamp = {Mon, 13 Aug 2018 16:48:42 +0200}, 
}

@inproceedings{benchmarkingRL,
  author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  title = {Benchmarking Deep Reinforcement Learning for Continuous Control},
  booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
  series = {ICML'16},
  year = {2016},
  location = {New York, NY, USA},
  pages = {1329--1338},
  numpages = {10},
  acmid = {3045531},
  publisher = {JMLR.org},
}

@article{Hwangbo2017ControlOA,
  title={Control of a Quadrotor With Reinforcement Learning},
  author={Jemin Hwangbo and Inkyu Sa and Roland Siegwart and Marco Hutter},
  journal={IEEE Robotics and Automation Letters},
  year={2017},
  volume={2},
  pages={2096-2103}
}

@article{Qlearning,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@misc{
    pineau2020improving,
    title={Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)},
    author={Joelle Pineau and Philippe Vincent-Lamarre and Koustuv Sinha and Vincent Larivière and Alina Beygelzimer and Florence d'Alché-Buc and Emily Fox and Hugo Larochelle},
    year={2020},
    eprint={2003.12206},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{
    Ilyas2020A,
    title={A Closer Look at Deep Policy Gradients},
    author={Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
    booktitle={International Conference on Learning Representations},
    year={2020},
}

@misc{
    liu2019regularization,
    title={Regularization Matters in Policy Optimization},
    author={Zhuang Liu and Xuanlin Li and Bingyi Kang and Trevor Darrell},
    year={2019},
    eprint={1910.09191},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{
    shen2020deep,
    title={Deep Reinforcement Learning with Smooth Policy},
    author={Qianli Shen and Yan Li and Haoming Jiang and Zhaoran Wang and Tuo Zhao},
    year={2020},
    eprint={2003.09534},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@book{
    DSP, 
    author = {Proakis, John G. and Manolakis, Dimitris G.}, 
    title = {Digital Signal Processing (3rd Ed.): Principles, Algorithms, and Applications}, 
    year = {1996}, 
    isbn = {0133737624}, 
    publisher = {Prentice-Hall, Inc.}, 
    address = {USA} 
}

@inproceedings{repRL,
  title={State Representation Learning in Robotics: Using Prior Knowledge about Physical Interaction.},
  author={Jonschkowski, Rico and Brock, Oliver},
  booktitle={Robotics: Science and Systems},
  year={2014}
}

@article{repRL2,
  title={Learning state representations with robotic priors},
  author={Jonschkowski, Rico and Brock, Oliver},
  journal={Autonomous Robots},
  volume={39},
  number={3},
  pages={407--428},
  year={2015},
  publisher={Springer}
}


@article{Dextrous,
author = {OpenAI: Marcin Andrychowicz and Bowen Baker and Maciek Chociej and Rafal Józefowicz and Bob McGrew and Jakub Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and Jonas Schneider and Szymon Sidor and Josh Tobin and Peter Welinder and Lilian Weng and Wojciech Zaremba},
title ={Learning dexterous in-hand manipulation},
journal = {The International Journal of Robotics Research},
volume = {39},
number = {1},
pages = {3-20},
year = {2020},
doi = {10.1177/0278364919887447},

URL = { 
        https://doi.org/10.1177/0278364919887447
    
},
eprint = { 
        https://doi.org/10.1177/0278364919887447
    
}
,
    abstract = { We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object’s appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM. }
}


@inproceedings{
	RoboImitationPeng20,
	author = {Peng, Xue Bin and Coumans, Erwin and Zhang, Tingnan and Lee, Tsang-Wei Edward and Tan, Jie and Levine, Sergey},
	booktitle={Robotics: Science and Systems},
	year = {2020},
	month = {07},
	title = {Learning Agile Robotic Locomotion Skills by Imitating Animals},
	doi = {10.15607/RSS.2020.XVI.064}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{Lillicrap2016ContinuousCW,
  title={Continuous control with deep reinforcement learning},
  author={T. Lillicrap and J. Hunt and A. Pritzel and N. Heess and T. Erez and Y. Tassa and D. Silver and Daan Wierstra},
  journal={CoRR},
  year={2016},
  volume={abs/1509.02971}
}

@article{Ziegler1942OptimumSF,
  title={Optimum Settings for Automatic Controllers},
  author={J. G. Ziegler and N. B. Nichols},
  journal={Journal of Dynamic Systems Measurement and Control-transactions of The Asme},
  year={1942},
  volume={115},
  pages={220-222}
}

@article{scaman2018lipschitz,
  title={Lipschitz regularity of deep neural networks: analysis and efficient estimation},
  author={Scaman, Kevin and Virmaux, Aladin},
  journal={arXiv preprint arXiv:1805.10965},
  year={2018}
}

@article{miyato2018spectral,
  title={Spectral normalization for generative adversarial networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  journal={arXiv preprint arXiv:1802.05957},
  year={2018}
}

@inproceedings{cisse2017parseval,
  title={Parseval networks: Improving robustness to adversarial examples},
  author={Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={854--863},
  year={2017},
  organization={PMLR}
}
@misc{stable-baselines3,
  author = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah},
  title = {Stable Baselines3},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/stable-baselines3}},
}

@misc{TFAgents,
  title = { {TF-Agents}: A library for Reinforcement Learning in TensorFlow},
  author = {Sergio Guadarrama and Anoop Korattikara and Oscar Ramirez and
     Pablo Castro and Ethan Holly and Sam Fishman and Ke Wang and
     Ekaterina Gonina and Neal Wu and Efi Kokiopoulou and Luciano Sbaiz and
     Jamie Smith and Gábor Bartók and Jesse Berent and Chris Harris and
     Vincent Vanhoucke and Eugene Brevdo},
  url = "https://github.com/tensorflow/agents",
  year = 2
}

@INPROCEEDINGS{MetaSimToReal,
  author={Arndt, Karol and Hazara, Murtaza and Ghadirzadeh, Ali and Kyrki, Ville},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Meta Reinforcement Learning for Sim-to-real Domain Adaptation}, 
  year={2020},
  volume={},
  number={},
  pages={2725-2731},
  doi={10.1109/ICRA40945.2020.9196540}
}

@article{PasikDuncan1996AdaptiveC,
  title={Adaptive Control},
  author={Bozenna Pasik-Duncan},
  journal={IEEE Control Systems},
  year={1996},
  volume={16},
  pages={87-}
}

@article{Nagabandi2019LearningTA,
  title={Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning},
  author={Anusha Nagabandi and Ignasi Clavera and Simin Liu and Ronald S. Fearing and P. Abbeel and Sergey Levine and Chelsea Finn},
  journal={arXiv: Learning},
  year={2019}
}

@inproceedings{Song2020ProvablyEM,
  title={Provably Efficient Model-based Policy Adaptation},
  author={Yuda Song and Aditi Mavalankar and Wen Sun and Sicun Gao},
  booktitle={ICML},
  year={2020}
}

@inproceedings{model_quad_adapt,
author = {Schreier, Matthias},
year = {2012},
month = {08},
pages = {},
title = {Modeling and Adaptive Control of a Quadrotor},
doi = {10.1109/ICMA.2012.6282874}
}

@article{Cheng_Orosz_Murray_Burdick_2019, title={End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4213}, DOI={10.1609/aaai.v33i01.33013387}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Cheng, Richard and Orosz, Gábor and Murray, Richard M. and Burdick, Joel W.}, year={2019}, month={Jul.}, pages={3387-3395} }

@inproceedings{
eysenbach2018diversity,
title={Diversity is All You Need: Learning Skills without a Reward Function},
author={Benjamin Eysenbach and Abhishek Gupta and Julian Ibarz and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SJx63jRqFm},
}

@inproceedings{NEURIPS2018_a2802cad,
 author = {Hong, Zhang-Wei and Shann, Tzu-Yun and Su, Shih-Yang and Chang, Yi-Hsiang and Fu, Tsu-Jui and Lee, Chun-Yi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Diversity-Driven Exploration Strategy for Deep Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2018/file/a2802cade04644083dcde1c8c483ed9a-Paper.pdf},
 volume = {31},
 year = {2018}
}

misc{https://doi.org/10.48550/arxiv.1910.07113,
  doi = {10.48550/ARXIV.1910.07113},
  
  url = {https://arxiv.org/abs/1910.07113},
  
  author = {{OpenAI} and Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Solving Rubik's Cube with a Robot Hand},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}